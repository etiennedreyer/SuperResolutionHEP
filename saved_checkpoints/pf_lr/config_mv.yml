pf_model:

    init_weights:
        all_linear: xavier_uniform # xavier_uniform | null # 1
        layer_emb_table: normal # normal | null # 2
        ln_modulation: zero # zero | null # 4

    h_dim: 64
    max_particles: 4
    
    encoder:
        layer_emb_dim: 4

        transformer:
            type: 'DiT' # 'GPT-2+Normformer' | 'DiT'
            num_heads: 4
            num_transformer_layers: 3
            dense_config: 
                hidden_layers: [64]
                activation: LeakyReLU
                final_activation: null
                norm_layer: LayerNorm
                norm_final_layer: false
                dropout: 0.0
                context_size: 0
            context_size: 64 # global representation of the cells features


    cardinality_predictor:
        input_size: 64
        output_size: null
        hidden_layers: [128, 64, 32]
        activation: "LeakyReLU"
        final_activation: null
        norm_layer: "LayerNorm"
        norm_final_layer: false
        dropout: 0.0


    kinematics_predictor:
        init_particles:
            type: 'embedding'
            embedding_dim: 4

        # init_particles:
        #     type: 'random'

        transformer:
            type: 'DiT' # 'GPT-2+Normformer' | 'DiT'
            num_heads: 4
            num_transformer_layers: 3
            dense_config: 
                hidden_layers: [64]
                activation: LeakyReLU
                final_activation: null
                norm_layer: LayerNorm
                norm_final_layer: false
                dropout: 0.0
            context_size: 64 # global representation of the cells features

        use_attn_kinematics: true # if true pt_eta_phi_e_net won't be used
        # pt_eta_phi_e_net:
        #     input_size: 64
        #     output_size: 4
        #     hidden_layers: [128, 64, 32]
        #     activation: "LeakyReLU"
        #     final_activation: null
        #     norm_layer: "LayerNorm"
        #     norm_final_layer: false
        #     dropout: 0.0


var_transform:
    eta: {
        "transformation": null,
        "scale_mode": "min_max",
        "mean": null, "std": null,
        "min": -2.988, "max": 2.988, "range": [-1,1]}

    e: {
        "transformation": "pow(x,m)",
        "m": 0.5,
        "scale_mode": "standard",
        "mean": 7.35, "std": 15.65, # MeV
        "min": 1.0, "max": 354.27, "range": [-1,1]} # MeV

    pt: {
        "transformation": "pow(x,m)",
        "m": 0.5,
        "scale_mode": "standard",
        "mean": 7.35, "std": 15.65, # MeV
        "min": 1.0, "max": 354.27, "range": [-1,1]} # MeV
